import torch
import os
import torch.nn as nn
import numpy as np
import cv2
import torchvision.transforms as transforms
import  pandas as pd
from torch.utils.data import DataLoader,Dataset
import time

# Read image
# 利用OpenCV(cv2)读入照片并存放在numpy array中


def readfile(path,label):
    # label是一个boolean variab,代表需不需要回传y值
    image_dir=sorted(os.listdir(path))
    x=np.zeros((len(image_dir), 128, 128, 3),dtype=np.uint8)
    y=np.zeros((len(image_dir)),dtype=np.uint8)
    for i,file in enumerate(image_dir):
        img=cv2.imread(os.path.join(path,file))
        x[i, :, :]=cv2.resize(img, (128, 128))
        if label:
            y[i]=int(file.split("_")[0])
    if label:
        return x,y
    else:
        return x
# 分别将training set、validation set、testing set用readfile函数读取


workspace_dir='./food-11'
print("Reading_data")
train_x, train_y=readfile(os.path.join(workspace_dir, "training"), True)
print("Size of training data={}".format(len(train_x)))
val_x, val_y=readfile(os.path.join(workspace_dir, "validation"), True)
print("Size of validation data ={}".format(len(val_x)))
test_x=readfile(os.path.join(workspace_dir, "testing"), False)
print("Szie of Testing data= {}".format(len(test_x)))

# DataSet
'''
在PyTorch中，我们可以利用torch.utils.data的DataSet及DataLoader来“包装”data，使后续的training及testing更为方便
DataSet需要overload两个函数:_len_及_getitem_
_len_必须要回传dataSet的大小，而_getitem_则定义了当程式利用[]取值时，dataSet应该要怎么回传资料
实际上我们并不会直接使用到这两个函数，但是使用DataLoader在enumerate DataSet时会使用到，没有实作的话会在程式运行阶段出现error
'''
# training时做data augmentation
trian_transform=transforms.Compose([
    transforms.ToPILImage(),
    transforms.RandomHorizontalFlip(), # 随机将图片水平翻转
    transforms.RandomRotation(), # 随机旋转图片
    transforms.ToTensor(), # 将图片转成Tensor，并把数值normalize到[0,1]（data normalization）
])
# testing时不需做data augmentation
test_transform=transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
])

class ImgDataset(Dataset):

    def __init__(self,x,y=None, transform=None):
        self.x=x
        # label is required to be a LongTensor
        self.y=y
        if y is not None:
            self.y=torch.LongTensor(y)
        self.transform=transform

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        X=self.x[index]
        if self.transform is not None:
            X=self.transform(X)
        if self.y is not None:
            Y=self.y[index]
            return X,Y
        else:
            return X
batch_size=128
train_set=ImgDataset(train_x,train_y,trian_transform)
val_set=ImgDataset(val_x,val_y,test_transform)
train_loader=DataLoader(train_set,batch_size=batch_size,shuffle=True)
val_loader=DataLoader(val_set,batch_size=batch_size,shuffle=False)

# Model


class Classfier(nn.Module):

    def __init__(self):
        super(Classfier,self).__init__()
        # torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)
        # torch.nn.MaxPool2d(kenel_size,stride,padding)
        # input维度[3,128,128]
        self.cnn=nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1), # [64, 128, 128]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0), # [64, 64, 64]

            nn.Conv2d(64, 128, 3, 1, 1),  # [128, 64, 64]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # [128, 32, 32]

            nn.Conv2d(128, 256, 3, 1, 1),  # [256, 32, 32]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # [256, 16, 16]

            nn.Conv2d(256, 512, 3, 1, 1),  # [512, 16, 16]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # [512, 8, 8]

            nn.Conv2d(512, 512, 3, 1, 1),  # [512, 8, 8]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # [512, 4, 4]
        )
        self.fc=nn.Sequential(
            nn.Linear(512*4*4, 1024),
            nn.ReLU(),
            nn.Linear(1024,512),
            nn.ReLU(),
            nn.Linear(512,11)
        )
    def forwad(self,x):
        out=self.cnn(x)
        out=out.view(out.size()[0],-1)
        return self.fc(out)
# Training
# 使用training set训练，并使用validation set寻找好的参数


model=Classfier().cuda()
loss=nn.CrossEntropyLoss() # 因为是classification task，所以loss使用CrossEntropyLoss
optimizer=torch.optim.Adam(model.parameters(), lr=0.001) # optimizer使用Adam
num_epoch=30
for epoch in range(num_epoch):
    epoch_start_time=time.time()
    train_acc=0.0
    train_loss=0.0
    val_acc=0.0
    val_loss=0.0

    model.train() # 确保model是在 train model(开启Dropout 等...)
    for i,data in enumerate(train_loader):
        optimizer.zero_grad() # 用optimizer将model 参数的gradient归零
        train_pred=model(data[0].cuda) # 利用model得到预测的机率分布 这边实际上就是去调用model 的forward函数
        batch_loss=loss(train_pred,data[1].cuda()) # 计算loss（注意prediction跟label必须同时在GPU或是CPU上）
        batch_loss.backward() # 利用back propagation算出每个参数的gradient
        optimizer.step() # 以optimizer 用gradient 更新参数值

        train_acc+=np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1)== data[1].numpy())
        train_loss+=batch_loss.item()

        model.eval()
        with torch.no_grad():
            for i, data in enumerate(val_loader):
                val_pred=model(data[0].cuda())
                batch_loss=loss(val_pred,data[1].cuda())

                val_acc+=np.sum(np.argmax(val_pred.cpu(),data.numpy(), axis=1)== data[1].numpy())
                val_loss+=batch_loss.item()

                # 将结果 print 出来
                print('[%03d/%03d]%2.2f sec(s) Train Acc: %3.6f Loss: %3.6f|Val Acc: %3.6f loss: %3.6f'%
                      (epoch+1, num_epoch, time.time()-epoch_start_time, train_acc/train_set.__len__(), train_loss/train_set.__len__(),
                       val_acc/val_set.__len__(), val_loss/val_set.__len__())
                      )
train_val_x=np.concatenate((train_x, val_x), axis=0)
train_val_y=np.concatenate((train_y, val_y), axis=0)
train_val_set=ImgDataset(train_val_x, train_val_y, trian_transform)
train_val_loader=DataLoader(train_val_set, batch_size=batch_size, shuffle=True)

model_best=Classfier().cuda()
loss=nn.CrossEntropyLoss() # 因为是classification task，所以loss使用CrossEntropyLoss
optimizer=torch.optim.Adam(model.parameters(), lr=0.001) # optimizer使用Adam
num_epoch=30
for epoch in range(num_epoch):
    epoch_start_time=time.time()
    train_acc=0.0
    train_loss=0.0

    model_best.eval()
    for i,data in enumerate(train_val_loader):
        optimizer.zero_grad() # 用optimizer将model 参数的gradient归零
        train_pred=model(data[0].cuda) # 利用model得到预测的机率分布 这边实际上就是去调用model 的forward函数
        batch_loss=loss(train_pred,data[1].cuda()) # 计算loss（注意prediction跟label必须同时在GPU或是CPU上）
        batch_loss.backward() # 利用back propagation算出每个参数的gradient
        optimizer.step() # 以optimizer 用gradient 更新参数值

        train_acc+=np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1)== data[1].numpy())
        train_loss+=batch_loss.item()

        # 将结果 print 出来
        print('[%03d/%03d]%2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' %\
              (epoch + 1, num_epoch, time.time() - epoch_start_time, train_acc / train_set.__len__(),
               train_loss / train_set.__len__()))
# Testing


test_set=ImgDataset(test_x, transform=test_transform)
test_loader=DataLoader(test_set, batch_size=batch_size, shuffle=False)
model_best.eval()
prediction=[]
with torch.no_grad():
    for i, data in enumerate(test_loader):
        test_pred=model_best(data.cuda())
        test_label=np.argmax(test_pred.cpu().data.numpy(), axis=1)
        for y in test_label:
            prediction.append(y)
    
    # 将结果写入 csv 
    with open("predict.csv", "w") as f:
        f.write('Id, Category\n')
        for i, y in enumerate(prediction):
            f.write('{}, {}\n'.format(i, y))
